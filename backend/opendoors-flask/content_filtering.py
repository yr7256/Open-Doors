'''
ì‘ì„± ëª©ë¡
1. ê±°ë¦¬ê³„ì‚° ë§¨í•˜íƒ„ê±°ë¦¬. í•˜ë²„ì‚¬ì¸ê±°ë¦¬ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ëŒ€ì²´ (kmë¡œ ë‚˜ì˜´)

2. ì‹œì„¤ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0-1 ì‚¬ì´ ìœ ì‚¬ë„ë¡œ ë‚˜ì˜´)
2-1. ì‹œì„¤ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ìµœì í™” ìœ„í•œ ìœ ì €ì •ë³´ë°°ì—´ 0ê°’ ì œê±° ë¡œì§

3. 
'''

from sklearn.metrics.pairwise import cosine_similarity
from haversine import haversine
from math import radians, log10

import numpy as np
import pandas as pd


# í†µí•©ëœ matrixê°€ ë“¤ì–´ì˜¤ë‹ˆê¹Œ ìª¼ê°œê³ , ë¶„ë¥˜í•´ì„œ ê¸°ëŠ¥ì œê³µ. ğŸ˜€ pkë§¤í•‘ ìœ ì§€ í•´ì•¼ë¨.
def content_based_recom(ref_arr, spot_matrix, category=None):
    # ref_arr = [1, 1,0,0,0,0,0,0,0, 36.3965, 127.4027, 4.49, 244]

    cat_col_num = 13 # ë§¨ ë§ˆì§€ë§‰ì— ë¼ì›Œë„£ì„ê²ƒì„.
    # spot_matrixì˜ catì´ 1(ì¹´í˜)ì¸ ê³³ë“¤ë§Œ ì„ íƒ
    spot_df = pd.DataFrame(spot_matrix)

    ref_spotId = ref_arr[0]
    ref_facility_arr = ref_arr[1:9]
    ref_coor = ref_arr[9:11]
    ref_rating = ref_arr[11:13]

    # ì¹´í…Œê³ ë¦¬ì˜ ì •ë³´ê°€ ì¼ì¹˜í•˜ëŠ” rowë§Œ ì‚´ë¦° df
    if category != None: # ì¹´í…Œê³ ë¦¬ì— 0ë„ ìˆìŒ.
        cat_filtered_df = spot_df.loc[spot_df.iloc[:, cat_col_num] == category, :]
    else:
        cat_filtered_df = spot_df

    facility_spotIds = cat_filtered_df.iloc[:, :1]
    
    
    facility_spotIds = [item[0] for item in facility_spotIds.values.tolist()] # nested listë¡œ ë°›ì•„ì˜¨ê±¸ arrë¡œ ë³€í™˜
    
    facility_df = cat_filtered_df.iloc[:, 1:9]
    coor_df = cat_filtered_df.iloc[:, 9:11]
    rating_df = cat_filtered_df.iloc[:, 11:13]
    matrix_size = len(facility_spotIds)
    
    # 1ì°¨ - ì‹œì„¤ ìœ ì‚¬ë„ì •ë³´ êµ¬í•¨. ndArr.
    facility_scores = facility_cos_sim(ref_facility_arr, facility_df) # 0-10ì˜ ìŠ¤ì½”ì–´ê°€ ë‚˜ì˜¨ë‹¤.
    

    # ê¸°ì¤€ ì¢Œí‘œì •ë³´ë¡œë¶€í„° ê° ì‹œì„¤ì˜ ë§¨í•˜íƒ„ê±°ë¦¬ë¥¼ êµ¬í•œ list
    manhattan_distances = [manhattan_distance(ref_coor, coor_item) for coor_item in coor_df.itertuples(index=False)]
    manhattan_scores = convert_manhattan_distances(manhattan_distances) # 0-10ì˜ ìŠ¤ì½”ì–´ê°€ ë‚˜ì˜¨ë‹¤.
    
    # rating_scores = [rating_score(rating_df[idx][0], rating[idx][1]) for idx in range(matrix_size)]
    rating_scores = [rating_score(*rating) for rating in rating_df.itertuples(index=False)] # 0-10ì˜ ìŠ¤ì½”ì–´ê°€ ë‚˜ì˜¨ë‹¤.
    
    # ìœ„ì˜ ì‹œì„¤ìœ ì‚¬ë„, ë§¨í•˜íƒ„ê±°ë¦¬, rating_score ë°˜ì˜ëœê±¸ ì·¨í•© í›„, ìƒìœ„ 10ê°œ ë°˜í™˜.
    scores_sum = sum_scores(facility_scores, manhattan_scores, rating_scores) # 0-30ì˜ ìŠ¤ì½”ì–´ê°€ ë‚˜ì˜¨ë‹¤.

    score_id_mapped_list = [(score/30, spotId, manhattan_dist) for score, spotId, manhattan_dist in zip(scores_sum, facility_spotIds, manhattan_distances)] # 
    # print('ìµœì¢…ë³€í™˜ë¦¬ìŠ¤íŠ¸')
    
    # res = sorted(score_id_mapped_list, reverse=True)
    # [(í™˜ì‚°í•©ì‚°ì ìˆ˜0-1, pk, ë§¨í•˜íƒ„ê±°ë¦¬)...] ë¡œ ë˜ì–´ìˆëŠ” ëª¨ë“  ì¥ì†Œì˜ì˜ ë°°ì—´ì´ ë‚˜ì˜´. top10ê°œë¡œ ì¶”ë¦¬ëŠ” ê³¼ì • í•„ìš”.
    return score_id_mapped_list, manhattan_distances, facility_scores
    



def binary_vectorize(arr):
    # 8ê°œì§œë¦¬ vector ë°°ì—´ë§Œë“¬
    bin_vector = np.zeros(8)
    bin_vector[np.array(arr)-1] = 1
    return bin_vector


# ê±°ë¦¬ê³„ì‚° 1 - ë§¨í•˜íƒ„ê±°ë¦¬
def manhattan_distance(coor_A, coor_B):
    """
    ë‘ ì§€ì ì˜ ìœ„ë„ì™€ ê²½ë„ë¥¼ ì…ë ¥ë°›ì•„ ë§¨í•˜íƒ„ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    ê¸°ë³¸ ë‹¨ìœ„ëŠ” kmì¸ë° ì¶”í›„ scale ì¡°ì ˆ.
    """
    a_lng, a_lat = coor_A
    b_lng, b_lat = coor_B
    coor_midpoint = [b_lng, a_lat]

    lng_dist = haversine(coor_A, coor_midpoint)
    lat_dist = haversine(coor_B, coor_midpoint)
    
    # ë§¨í•˜íƒ„ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    sum_distance = lng_dist + lat_dist
    return sum_distance


# ë§¨í•˜íƒ„ê±°ë¦¬ ìŠ¤ì½”ì–´ë¡œ ë³€í™˜. 500më¯¸ë§Œ ë§Œì . 10kmì´ˆê³¼ 0ì . ì‚¬ì´ëŠ” ê±°ë¦¬ì— ë°˜ë¹„ë¡€
def convert_manhattan_distances(manhattan_distances):
    result = []

    for distance in manhattan_distances:
        if distance < 0.5:  # 0.5 km ë¯¸ë§Œì¸ ê²½ìš°
            single_score = 10
        elif distance > 10:  # 10 km ì´ˆê³¼ì¸ ê²½ìš°
            single_score = 0
        else:  # 0.5 km ì´ìƒ 10 km ì´í•˜ì¸ ê²½ìš°
            single_score = ((1 - (distance - 0.5) / 9.5)*10)
        
        result.append(single_score)
    return result


# ì‹œì„¤ ìœ ì‚¬ë„ arrë¡œ ë°˜í™˜, idx ìœ ì§€
def facility_cos_sim(ref_facility_arr, facility_matrix):
    ref_facility_arr = np.array(ref_facility_arr).reshape(1,-1)
    res = cosine_similarity(ref_facility_arr, facility_matrix)
    return res[0]


# ê°€ì¤‘ì¹˜ ì¡°ì ˆ ì¶”í›„ì— ì§„í–‰
def rating_score(avg_score, count):
    score_weight = 1
    count_weight = 1

    return (avg_score*score_weight + log10(count)*count_weight)


def sum_scores(facility_scores, manhattan_scores, rating_scores):
    return [sum(score_tuple) for score_tuple in zip(facility_scores, manhattan_scores, rating_scores)]


# ì‹œì„¤ìœ ì‚¬ë„ - ì†ë„ê°œì„ 1 (field ì¶•ì†Œ)
def valid_field(ref_facility_arr):
    '''
    user_pref_arrì—ì„œ 0ì¸ idxë¥¼ ì‹¹ ë‚ ë ¤ë²„ë¦¼.
    ìœ íš¨í•œ field idxë§Œ ë¬¶ì–´ì„œ ë°˜í™˜
    '''
    pass

# ì‹œì„¤ìœ ì‚¬ë„ - ì†ë„ê°œì„ 1 (ì¶•ì†Œëœ field ë°˜ì˜)
def apply_valid_field(facility_matrix):
    '''
    valid_fieldì—ì„œ 0ìœ¼ë¡œ ë‚ ì•„ê°„ idxë¥¼ ì œê±°í•œ matrix ë°˜í™˜
    '''
    pass


# weighted_score